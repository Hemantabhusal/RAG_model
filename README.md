RAG-Powered Q&A System with FAISS and Groq
RAG-Powered Q&A System with Embeddings is a project designed to build an intelligent question-answering system that retrieves and processes information from text or PDF documents. The system leverages retrieval-augmented generation (RAG) to provide accurate, context-aware responses using advanced embedding and large language model (LLM) technologies.
Main Objectives

Generate Text Embeddings: Create embeddings for document chunks using HuggingFace’s all-MiniLM-L6-v2 model.
Manage a Vector Database: Store and retrieve document embeddings using FAISS for efficient similarity search.
Interactive Q&A: Allow users to ask questions and receive concise answers based on document content via Groq’s llama3-70b-8192 model.

Key Functionalities

Document Processing: Load and split text or PDF files into chunks (100 to 10,000 characters) for processing.
Embedding Generation: Represent text as numerical vectors using all-MiniLM-L6-v2.
Vector Storage and Retrieval: Store embeddings in FAISS and retrieve the most relevant chunks for queries.
Contextual Responses: Query the FAISS index and generate answers using the Groq API.

Overview
The project consists of four main scripts:

document_processor.py: Loads text or PDF files and splits them into chunks using langchain’s RecursiveCharacterTextSplitter.
embedding_generator.py: Generates embeddings for document chunks using all-MiniLM-L6-v2.
vector_store.py: Manages a FAISS vector database for storing and retrieving embeddings.
query_processor.py: Queries the FAISS index and interacts with the Groq API to answer questions.
main.py: Runs the command-line Q&A pipeline.

Features

Process text or PDF files of varying lengths (100 to 10,000 characters) into ~1 to 27 chunks.
Generate lightweight, CPU-friendly embeddings with all-MiniLM-L6-v2.
Store embeddings in FAISS for fast similarity search.
Query the system via command line with answers generated by Groq’s llama3-70b-8192.
Dynamic file input for flexible document processing.

Technologies Used

Embeddings: HuggingFace sentence-transformers (all-MiniLM-L6-v2)
Vector Database: FAISS
API: Groq (llama3-70b-8192)
Document Processing: langchain, PyPDF2
Environment Variables: python-dotenv

Installation
Follow these steps to set up the project on your local machine.
Prerequisites

Python version: Ensure Python 3.11 or higher is installed.

Setup

Clone the repository:
git clone https://github.com/your-username/rag-project.git
cd rag-project


Install Python dependencies: Create a requirements.txt file with the following content:
torch==2.0.1
transformers==4.35.0
sentence-transformers==2.2.2
langchain==0.0.340
faiss-cpu==1.7.4
pypdf2==3.0.1
requests==2.31.0
python-dotenv==1.0.1

Then, install the dependencies using pip:
pip install -r requirements.txt


Set up environment variables: Create a .env file in the project root with the following content:
GROQ_API_KE=your_groq_api_key


Create a data/ directory and add a text or PDF file (e.g., data/sample.txt).

Run the script:
To run the command-line Q&A system:
python main.py



Usage

Run main.py and enter a file path (e.g., data/sample.txt) when prompted.

Ask questions (e.g., "What is LLM" or "define backpropagation") to receive context-aware answers.

Type quit to exit.

Example output:
Enter file path (e.g., data/sample.txt): data/sample.txt
INFO:__main__:Loaded API key: gsk_7tmnIE...figYLVhG
INFO:document_processor:Loaded 1 document from data/sample.txt, length: 1500 characters
INFO:document_processor:Split into 8 chunks
Chunk 1: Gradient descent is an optimization algorithm used to minimize a loss function in machine learning....
...
Enter your question (or 'quit' to exit): What is LLM
Question: What is LLM
Answer: Large language models (LLMs) are transformer-based neural networks trained on massive datasets to generate human-like text.
Context: ['Large language models (LLMs) are built on transformer architectures...', 'Retrieval-Augmented Generation (RAG) combines retrieval...']



Contributors

Hemanta Bhusal

Acknowledgements
Thanks to HuggingFace for providing the all-MiniLM-L6-v2 model, Groq for their llama3-70b-8192 API, and LangChain for their document processing and splitting libraries. Special thanks to FAISS for efficient vector storage.